# -*- coding:utf-8 -*-
"""
__author__ = "@YuweiYin"
"""

import os
import sys
import re
import datetime
import json
import pickle
import random
import numpy as np

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets.utils import check_integrity
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import _LRScheduler

from img_clf.dataset import CIFAR10, CIFAR100, CocoDataset, Office31, OfficeHome, CrossDomainSrcTgt
from img_clf.extra_transforms import MoCoAugTransforms, TwoCropsTransform


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def get_network_classification(args):
    assert hasattr(args, "num_classes") and isinstance(args.num_classes, int)
    if args.net == 'resnet18':
        from img_clf.models.resnet import resnet18
        net = resnet18(num_classes=args.num_classes)
    elif args.net == 'resnet34':
        from img_clf.models.resnet import resnet34
        net = resnet34(num_classes=args.num_classes)
    elif args.net == 'resnet50':
        from img_clf.models.resnet import resnet50
        net = resnet50(num_classes=args.num_classes)
    elif args.net == 'resnet101':
        from img_clf.models.resnet import resnet101
        net = resnet101(num_classes=args.num_classes)
    elif args.net == 'resnet152':
        from img_clf.models.resnet import resnet152
        net = resnet152(num_classes=args.num_classes)

    else:
        print(f'the network {args.net} you have entered is not supported yet')
        sys.exit()

    if args.gpu:  # use_gpu
        net = net.cuda()

    return net


def get_prompts(syn_data: str = "cifar100"):
    """get prompts (label text or descriptions/captions generated by the text-to-text model GPT) of each dataset"""
    if syn_data == "cifar100":
        # CIFAR-100 class names
        # str_labels = [
        #     'apples', 'aquarium fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottles',
        #     'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'cans', 'castle', 'caterpillar', 'cattle',
        #     'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cups',
        #     'dinosaur', 'dolphin', 'elephant', 'flat fish', 'forest', 'fox', 'girl', 'hamster', 'house',
        #     'kangaroo', 'computer keyboard', 'lamp', 'lawn mower', 'leopard', 'lion', 'lizard', 'lobster',
        #     'man', 'maple tree', 'motorcycle', 'mountain', 'mouse', 'mushrooms', 'oak tree', 'oranges',
        #     'orchid', 'otter', 'palm tree', 'pears', 'pickup truck', 'pine tree', 'plain', 'plate', 'poppy',
        #     'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'roses', 'sea', 'seal',
        #     'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar',
        #     'sunflowers', 'sweet peppers', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor',
        #     'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow tree', 'wolf', 'woman', 'worm'
        # ]
        # humans = [str_labels.index(each) for each in ["baby", "boy", "girl", "man", "woman", "plain"]]

        # CIFAR-100 meta info
        base_folder = "cifar-100-python"
        # url = "https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz"
        # filename = "cifar-100-python.tar.gz"
        # tgz_md5 = "eb9058c3a382ffc7106e4002c42a8d85"
        # train_list = [
        #     ["train", "16019d7e3df5f24257cddd939b257f8d"],
        # ]
        #
        # test_list = [
        #     ["test", "f0ef6b0ae62326f3e7ffdfab6717acfc"],
        # ]
        meta = {
            "filename": "meta",
            "key": "fine_label_names",
            "md5": "7973b15100ade9c7d40fb424638fde48",
        }

        path_cifar = os.path.join("./data", base_folder, meta["filename"])
        if not check_integrity(path_cifar, meta["md5"]):
            raise RuntimeError(
                "Dataset metadata file not found or corrupted. You can use download=True to download it")
        with open(path_cifar, "rb") as infile:
            data = pickle.load(infile, encoding="latin1")
        prompts = data[meta["key"]]
        prompts.sort()
    elif syn_data == "office_31":  # "amazon", "dslr", "webcam"  31 classes
        syn_dir = os.path.join("./data/cross_domain/", syn_data, "amazon")
        assert os.path.isdir(syn_dir)
        prompts = os.listdir(syn_dir)
        prompts.sort()
    elif syn_data == "office_home":  # "Art", "Clipart", "Product", "RealWorld"  65 classes
        syn_dir = os.path.join("./data/cross_domain/", syn_data, "Art")
        assert os.path.isdir(syn_dir)
        prompts = os.listdir(syn_dir)
        prompts.sort()
    elif syn_data == "coco_cap":
        # use the original COCO captions as prompts for the T2I model GLIDE
        from pycocotools.coco import COCO

        # initialize COCO API for instance annotations
        coco_data_root = "./data/COCO_2015_Captioning/"
        coco_train_cap = COCO(os.path.join(coco_data_root, "annotations/captions_train2014.json"))
        # coco_train_ins = COCO(os.path.join(coco_data_root, "annotations/instances_train2014.json"))
        # coco_train_per = COCO(os.path.join(coco_data_root, "annotations/person_keypoints_train2014.json"))
        # coco_valid_cap = COCO(os.path.join(coco_data_root, "annotations/captions_val2014.json"))
        # coco_valid_ins = COCO(os.path.join(coco_data_root, "annotations/instances_val2014.json"))
        # coco_valid_per = COCO(os.path.join(coco_data_root, "annotations/person_keypoints_val2014.json"))

        coco_train_cap_ids = list(coco_train_cap.anns.keys())
        prompts = [coco_train_cap.anns[cap_id]['caption'] for cap_id in coco_train_cap_ids]
        prompts = [prompt.strip() for prompt in prompts]

        prompts.sort()
    elif syn_data == "coco_cap_ner":
        # extract NER from original COCO captions, and then use NER as prompts for the T2I model GLIDE
        gpt_dir = os.path.join("./data/gpt/")
        gpt_ner_data_path = os.path.join(gpt_dir, "gpt_ner_data.pt")
        assert os.path.isfile(gpt_ner_data_path), "Please run: `python run_coco_ner.py`"

        gpt_finetune_data = torch.load(gpt_ner_data_path)
        gpt_ner = gpt_finetune_data['gpt_ner']
        # gpt_text = gpt_finetune_data['gpt_text']
        # coco_anns = gpt_finetune_data['coco_anns']
        prompts = gpt_ner
        prompts = [prompt.strip() for prompt in prompts]
        prompts.sort()
    elif syn_data == "coco_cap_ner_gpt_sent":
        # extract NER from original COCO captions, and then use GPT-2 to generate sentences from the extracted NER,
        # the GPT-2 model has been fine-tuned on the original COCO captions for 5 epochs.
        save_sent_dir = f"./data/gpt/gpt2_sent_coco_cap_ner/"
        assert os.path.isdir(save_sent_dir), "Please run: `python run_gpt_generate.py --syn_data coco_cap_ner`"
        ckpt_epoch = 4
        ner_sent_filepath = os.path.join(save_sent_dir, f"gpt2_generation_epoch_{ckpt_epoch}.txt")
        with open(ner_sent_filepath, "r") as f_in:
            ner_sent_list = f_in.readlines()
        prompts = [ner_sent.strip().split("\t")[1] for ner_sent in ner_sent_list]
        prompts = [prompt.strip() for prompt in prompts]
        prompts.sort()
    else:
        raise ValueError(f"ValueError: syn_data = {syn_data}")

    print(f"len(prompts) = {len(prompts)}")
    # print(prompts[:100])
    return prompts


def get_dataloader_cifar(
        mean, std, ds_name="cifar10", batch_size=16, num_workers=2,
        shuffle=True, n_img=-1, n_img_syn=0, syn_type="glide_base",
        long_tail=False, adversarial=False, adv_as_test=False, extra_transforms=False):
    """ return training dataloader
    Args:
        mean: mean of cifar training dataset
        std: std of cifar training dataset
        ds_name: "cifar10" or "cifar100"
        batch_size: dataloader batch size
        num_workers: dataloader num_works
        shuffle: whether to shuffle
        n_img: the total number of images for training. -1: all
        n_img_syn: the number of synthetic images per class. -1: all. 0: none.
        syn_type: glide_base 64x64 or glide_upsample 256x256 or gan_base 64x64
        long_tail: use the long-tail subset for training or not
        adversarial: add adversarial attacking images to the training set or not
        adv_as_test: use adv images as test set or not
        extra_transforms: use transforms method in MoCo or not
    Returns: train_data_loader: torch dataloader object
    """

    if extra_transforms:
        moco_transforms = MoCoAugTransforms(mean, std)
        # augmentation = moco_transforms.augmentation_v1
        augmentation = moco_transforms.augmentation_v2
        # transform_train = TwoCropsTransform(transforms.Compose(augmentation))
        transform_train = transforms.Compose(augmentation)
    else:
        transform_train = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])

    transform_val = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])

    if ds_name == "cifar10":
        cifar_train = CIFAR10(
            root='../data', train="train", download=True, transform=transform_train, n_img=n_img,
            syn_data="cifar10", n_img_syn=n_img_syn, syn_type=syn_type, long_tail=long_tail,
            adversarial=adversarial, adv_as_test=False)
        cifar_train_loader = DataLoader(
            cifar_train, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

        cifar_val = CIFAR10(
            root='../data', train="val", download=True, transform=transform_val, n_img=n_img,
            syn_data="cifar10", n_img_syn=0, syn_type=syn_type, long_tail=False,
            adversarial=False, adv_as_test=False)
        cifar_val_loader = DataLoader(
            cifar_val, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

        cifar_test = CIFAR10(
            root='../data', train="test", download=True, transform=transform_test, n_img=-1,
            syn_data="cifar10", n_img_syn=0, syn_type=syn_type, long_tail=False,
            adversarial=False, adv_as_test=adv_as_test)
        cifar_test_loader = DataLoader(
            cifar_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)
    elif ds_name == "cifar100":
        cifar_train = CIFAR100(
            root='../data', train="train", download=True, transform=transform_train, n_img=n_img,
            syn_data="cifar100", n_img_syn=n_img_syn, syn_type=syn_type, long_tail=long_tail,
            adversarial=adversarial, adv_as_test=False)
        cifar_train_loader = DataLoader(
            cifar_train, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

        cifar_val = CIFAR100(
            root='../data', train="val", download=True, transform=transform_val, n_img=n_img,
            syn_data="cifar100", n_img_syn=0, syn_type=syn_type, long_tail=False,
            adversarial=False, adv_as_test=False)
        cifar_val_loader = DataLoader(
            cifar_val, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

        cifar_test = CIFAR100(
            root='../data', train="test", download=True, transform=transform_test, n_img=-1,
            syn_data="cifar100", n_img_syn=0, syn_type=syn_type, long_tail=False,
            adversarial=False, adv_as_test=adv_as_test)
        cifar_test_loader = DataLoader(
            cifar_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)
    else:
        raise ValueError(f"error parameter ds_name: {ds_name}")

    return cifar_train_loader, cifar_val_loader, cifar_test_loader


def get_dataloader_cross_domain_src_tgt(
        data, src_domain, tgt_domain, mean, std,
        batch_size=16, num_workers=2, shuffle=True, n_img=-1, n_img_syn=0):
    """ return training dataloader
    data in ["office_31", "office_hone", "imageclef_da"]
    office_31.domain in ["amazon", "dslr", "webcam"]
    office_hone.domain in ["Art", "Clipart", "Product", "RealWorld"]
    """

    # preprocess = transforms.Compose([
    #     transforms.Resize(256),
    #     transforms.CenterCrop(224),
    #     transforms.ToTensor(),
    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # ])

    if mean is None or std is None:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor()
        ])
    else:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])

    root_dir = f"../data/cross_domain/{data}/"
    assert os.path.isdir(root_dir)

    office_31_training = CrossDomainSrcTgt(
        data=data, src_domain=src_domain, tgt_domain=tgt_domain, root=root_dir,
        transform=transform_train, n_img=n_img, n_img_syn=n_img_syn)

    office_31_training_loader = DataLoader(
        office_31_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

    return office_31_training_loader


def get_dataloader_office_31(
        domain, mean, std, batch_size=16, num_workers=2, shuffle=True, n_img=-1, n_img_syn=0):
    """ return training dataloader
    "amazon", "dslr", "webcam"
    """

    # preprocess = transforms.Compose([
    #     transforms.Resize(256),
    #     transforms.CenterCrop(224),
    #     transforms.ToTensor(),
    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # ])

    if mean is None or std is None:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor()
        ])
    else:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])

    office_31_training = Office31(
        domain=domain, root='../data/cross_domain/office_31/',
        transform=transform_train, n_img=n_img, n_img_syn=n_img_syn)
    office_31_training_loader = DataLoader(
        office_31_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

    return office_31_training_loader


def get_dataloader_office_home(
        domain, mean, std, batch_size=16, num_workers=2, shuffle=True, n_img=-1, n_img_syn=0):
    """ return training dataloader
    "Art", "Clipart", "Product", "RealWorld"
    """

    # preprocess = transforms.Compose([
    #     transforms.Resize(256),
    #     transforms.CenterCrop(224),
    #     transforms.ToTensor(),
    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    # ])

    if mean is None or std is None:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor()
        ])
    else:
        transform_train = transforms.Compose([
            # transforms.ToPILImage(),
            # transforms.RandomCrop(224),
            # transforms.RandomCrop(300, padding=4),
            # transforms.RandomCrop(32, padding=4),
            # transforms.RandomHorizontalFlip(),
            # transforms.RandomRotation(15),
            transforms.ToTensor(),
            transforms.Normalize(mean, std)
        ])

    office_home_training = OfficeHome(
        domain=domain, root='../data/cross_domain/office_home/',
        transform=transform_train, n_img=n_img, n_img_syn=n_img_syn)
    office_home_training_loader = DataLoader(
        office_home_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

    return office_home_training_loader


def get_training_dataloader_coco_captions(
        mean, std, batch_size=16, num_workers=2, shuffle=True, n_img=-1, n_img_syn=0):
    """ return training dataloader
    Args:
        mean: mean of coco_captions training dataset
        std: std of coco_captions training dataset
        batch_size: dataloader batch size
        num_workers: dataloader num_works
        shuffle: whether to shuffle
        n_img: the total number of images for testing. -1: all
        n_img_syn: the number of synthetic images per class. -1: all. 0: none.
    Returns: train_data_loader: torch dataloader object
    """

    # filter images with people, because GLIDE is not good at generating images of people
    # few shot setting (e.g., use 10% training set of coco dataset to fine-tune GLIDE. test on coco dev/test set)
    # exp on image captioning / fine tune GLIDE

    if mean is None or std is None:
        transform_train = transforms.Compose([
            transforms.Resize(256),  # smaller edge of image resized to 256
            transforms.RandomCrop(224),  # get 224x224 crop from random location
            transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
            transforms.ToTensor(),  # convert the PIL Image to a tensor
        ])
    else:
        transform_train = transforms.Compose([
            transforms.Resize(256),  # smaller edge of image resized to 256
            transforms.RandomCrop(224),  # get 224x224 crop from random location
            transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
            transforms.ToTensor(),  # convert the PIL Image to a tensor
            transforms.Normalize(mean=mean, std=std)  # normalize image for pre-trained model
        ])

    # vocab_threshold = 5  # the minimum word count threshold.
    vocab_threshold = 4  # the minimum word count threshold.

    data_loader, dataset = get_coco_loader(transform=transform_train,
                                           mode='train',
                                           batch_size=batch_size,
                                           vocab_threshold=vocab_threshold,
                                           vocab_from_file=True,
                                           shuffle=shuffle,
                                           num_workers=num_workers,
                                           n_img=n_img,
                                           n_img_syn=n_img_syn)

    return data_loader, dataset


def get_test_dataloader_coco_captions(
        mean, std, batch_size=16, num_workers=2, shuffle=False, n_img=-1, n_img_syn=0):
    """ return training dataloader
    Args:
        mean: mean of coco_captions test dataset
        std: std of coco_captions test dataset
        batch_size: dataloader batch size
        num_workers: dataloader num_works
        shuffle: whether to shuffle
        n_img: the total number of images for testing. -1: all
        n_img_syn: the number of synthetic images per class. -1: all. 0: none.
    Returns: test_data_loader: torch dataloader object
    """

    if mean is None or std is None:
        transform_valid = transforms.Compose([
            transforms.Resize(256),  # smaller edge of image resized to 256
            transforms.RandomCrop(224),  # get 224x224 crop from random location
            transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
            transforms.ToTensor(),  # convert the PIL Image to a tensor
        ])
    else:
        transform_valid = transforms.Compose([
            transforms.Resize(256),  # smaller edge of image resized to 256
            transforms.RandomCrop(224),  # get 224x224 crop from random location
            transforms.RandomHorizontalFlip(),  # horizontally flip image with probability=0.5
            transforms.ToTensor(),  # convert the PIL Image to a tensor
            transforms.Normalize(mean=mean, std=std)  # normalize image for pre-trained model
        ])

    # vocab_threshold = 5  # the minimum word count threshold.
    vocab_threshold = 4  # the minimum word count threshold.

    data_loader, dataset = get_coco_loader(transform=transform_valid,
                                           mode='val',
                                           batch_size=batch_size,
                                           vocab_threshold=vocab_threshold,
                                           vocab_from_file=True,
                                           shuffle=shuffle,
                                           num_workers=num_workers,
                                           n_img=n_img,
                                           n_img_syn=n_img_syn)

    return data_loader, dataset


def get_coco_loader(
        transform, mode="train", batch_size=1, vocab_threshold=4,
        pad_word="<pad>", bos_word="<bos>", eos_word="<eos>", unk_word="<unk>",
        vocab_from_file=True, vocab_file_path="../data/COCO_2015_Captioning/vocab.pkl",
        shuffle=True, num_workers=0, coco_root="../data/COCO_2015_Captioning/",
        n_img=-1, n_img_syn=0):
    """Return the data loader.
    Parameters:
        transform: Image transform.
        mode: One of "train", "val" or "test".
        batch_size: Batch size (if in testing mode, must have batch_size=1).
        vocab_threshold: Minimum word count threshold.
        pad_word: Special word denoting padding words.
        bos_word: Special word denoting a sentence starts.
        eos_word: Special word denoting a sentences ends.
        unk_word: Special word denoting unknown words.
        vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.
                If True, load vocab from existing vocab_file, if it exists.
        vocab_file_path: File containing the vocabulary.
        shuffle: Ture if shuffle the samples in the dataloader.
        num_workers: Number of subprocesses to use for data loading
        coco_root: the root directory of downloaded COCO dataset
        # cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi
        n_img: the total number of images for testing. -1: all
        n_img_syn: the number of synthetic images per class. -1: all. 0: none.
    """

    assert mode in ["train", "val", "test"], "mode must be one of 'train', 'val' or 'test'."
    if not vocab_from_file:
        assert mode == "train", "To generate vocab from captions file, \
               must be in training mode (mode='train')."

    # Based on mode (train, val, test), obtain img_folder and annotations_file
    if mode == "train":
        if vocab_from_file:
            assert os.path.isfile(vocab_file_path), "vocab_file does not exist.  \
                   Change vocab_from_file to False to create vocab_file."
        # img_folder = os.path.join(cocoapi_loc, "cocoapi/images/train2014/")
        # annotations_file = os.path.join(cocoapi_loc, "cocoapi/annotations/captions_train2014.json")
        img_folder = os.path.join(coco_root, "train2014/")
        annotations_file = os.path.join(coco_root, "annotations/captions_train2014.json")
    elif mode == "val":
        assert os.path.isfile(vocab_file_path), "Must first generate vocab.pkl from training data."
        assert vocab_from_file, "Change vocab_from_file to True."
        # img_folder = os.path.join(cocoapi_loc, "cocoapi/images/val2014/")
        # annotations_file = os.path.join(cocoapi_loc, "cocoapi/annotations/captions_val2014.json")
        img_folder = os.path.join(coco_root, "val2014/")
        annotations_file = os.path.join(coco_root, "annotations/captions_val2014.json")
    elif mode == "test":
        assert batch_size == 1, "Please change batch_size to 1 if testing your model."
        assert os.path.isfile(vocab_file_path), "Must first generate vocab.pkl from training data."
        assert vocab_from_file, "Change vocab_from_file to True."
        # img_folder = os.path.join(cocoapi_loc, "cocoapi/images/test2014/")
        # annotations_file = os.path.join(cocoapi_loc, "cocoapi/annotations/image_info_test2014.json")
        img_folder = os.path.join(coco_root, "test2014/")
        annotations_file = os.path.join(coco_root, "annotations/image_info_test2014.json")
    else:
        raise Exception(f"mode error: {mode}")

    # COCO caption dataset
    dataset = CocoDataset(transform=transform,
                          mode=mode,
                          batch_size=batch_size,
                          vocab_threshold=vocab_threshold,
                          pad_word=pad_word,
                          bos_word=bos_word,
                          eos_word=eos_word,
                          unk_word=unk_word,
                          annotations_file=annotations_file,
                          vocab_from_file=vocab_from_file,
                          vocab_file_path=vocab_file_path,
                          img_folder=img_folder,
                          n_img=n_img,
                          n_img_syn=n_img_syn)

    if mode == "train":
        # Randomly sample a caption length, and sample indices with that length.
        indices = dataset.get_indices()
        # Create and assign a batch sampler to retrieve a batch with the sampled indices.
        initial_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices=indices)
        # data loader for COCO dataset.
        data_loader = DataLoader(dataset=dataset,
                                 # shuffle=shuffle,
                                 num_workers=num_workers,
                                 batch_sampler=torch.utils.data.sampler.BatchSampler(
                                     sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False),
                                 collate_fn=collate_fn_coco)
    else:
        data_loader = DataLoader(dataset=dataset,
                                 batch_size=dataset.batch_size,
                                 shuffle=shuffle,
                                 num_workers=num_workers,
                                 collate_fn=collate_fn_coco)

    return data_loader, dataset


def collate_fn_coco(data):
    """Creates mini-batch tensors from the list of tuples (image, caption).

    We should build custom collate_fn rather than using default collate_fn,
    because merging caption (including padding) is not supported in default.
    Args:
        data: list of tuple (image, caption).
            - image: torch tensor of shape (3, 256, 256).
            - caption: torch tensor of shape (?); variable length.
    Returns:
        images: torch tensor of shape (batch_size, 3, 256, 256).
        targets: torch tensor of shape (batch_size, padded_length).
        lengths: list; valid length for each padded caption.
    """
    # Sort a data list by caption length (descending order).
    data.sort(key=lambda x: len(x[1]), reverse=True)
    # images, captions = zip(*data)
    images, captions, img_ids = zip(*data)

    # Merge images (from tuple of 3D tensor to 4D tensor).
    images = torch.stack(images, 0)

    # Merge captions (from tuple of 1D tensor to 2D tensor).
    lengths = [len(cap) for cap in captions]
    targets = torch.zeros(len(captions), max(lengths)).long()
    for i, cap in enumerate(captions):
        end = lengths[i]
        targets[i, :end] = cap[:end]
    lengths = torch.tensor(lengths, dtype=torch.long)
    return images, targets, img_ids, lengths


def compute_mean_std(cifar100_dataset):
    """compute the mean and std of cifar100 dataset
    Args:
        cifar100_training_dataset or cifar100_test_dataset
        witch derived from class torch.utils.data

    Returns:
        a tuple contains mean, std value of entire dataset
    """

    data_r = np.dstack([cifar100_dataset[i][1][:, :, 0] for i in range(len(cifar100_dataset))])
    data_g = np.dstack([cifar100_dataset[i][1][:, :, 1] for i in range(len(cifar100_dataset))])
    data_b = np.dstack([cifar100_dataset[i][1][:, :, 2] for i in range(len(cifar100_dataset))])
    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)
    std = np.std(data_r), np.std(data_g), np.std(data_b)

    return mean, std


class WarmUpLR(_LRScheduler):
    """warmup_training learning rate scheduler
    Args:
        optimizer: optimizer(e.g. SGD)
        total_iters: total_iters of warmup phase
    """

    def __init__(self, optimizer, total_iters, last_epoch=-1):
        self.total_iters = total_iters
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        """we will use the first m batches, and set the learning
        rate to base_lr * m / total_iters
        """
        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]


def most_recent_folder(net_weights, fmt):
    """
        return most recent created folder under net_weights
        if no none-empty folder were found, return empty folder
    """
    # get sub-folders in net_weights
    folders = os.listdir(net_weights)

    # filter out empty folders
    folders = [f for f in folders if len(os.listdir(os.path.join(net_weights, f)))]
    if len(folders) == 0:
        return ''

    # sort folders by folder created time
    folders = sorted(folders, key=lambda f: datetime.datetime.strptime(f, fmt))
    return folders[-1]


def most_recent_weights(weights_folder):
    """
        return most recent created weights file
        if folder is empty return empty string
    """
    weight_files = os.listdir(weights_folder)
    if len(weights_folder) == 0:
        return ''

    regex_str = r'([A-Za-z0-9]+)-([A-Za-z0-9]+)-(regular|best)-([0-9]+)'

    # sort files by epoch
    weight_files = sorted(weight_files, key=lambda w: int(re.search(regex_str, w).groups()[3]))

    return weight_files[-1]


def last_epoch(weights_folder):
    weight_file = most_recent_weights(weights_folder)
    if not weight_file:
        raise Exception('no recent weights were found')
    weight_filename = "".join(weight_file.split(".")[:-1])
    resume_epoch = int(weight_filename.split('-')[3])
    # print("resume_epoch:", resume_epoch)

    return resume_epoch


def best_acc_weights(weights_folder):
    """
        return the best acc .pth file in given folder, if no
        best acc weights file were found, return empty string
    """
    files = os.listdir(weights_folder)
    if len(files) == 0:
        return ''

    regex_str = r'([A-Za-z0-9]+)-([A-Za-z0-9]+)-(regular|best)-([0-9]+)'
    best_files = [w for w in files if re.search(regex_str, w).groups()[2] == 'best']
    if len(best_files) == 0:
        return ''

    best_files = sorted(best_files, key=lambda w: int(re.search(regex_str, w).groups()[3]))
    return best_files[-1]
